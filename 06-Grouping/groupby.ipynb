{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping\n",
    "\n",
    "Chapter 9 in Python for Data Analysis demonstrates a variety of methods to analyze data via data aggregation and grouping operations. Those are the focus of this session.  Our overall goal for this session is to do Exploratory Data Analysis, which is essentially looking at and probing our data to learn about the patterns we can discover in them.  Often this can generate a better understanding of problems in the data, as well as revealing relationships that might be worth exploring in more depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use 2010 US Census data, for all census tracts in Kentucky.  A good starting point for any kind of Census data is the American Fact Finder: \n",
    "\n",
    "https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml\n",
    "\n",
    "The Census Bureau produces several products, the most famous of which is the decennial census, which as its name implies, is a true Census.  There are a limited number of data fields for the whole population (or at least those that the Census workers are able to reach, which is pretty good).  The Census has other products that are surveys, such as the American Community Survey (ACS) which asks more detailed questions on an annual basis from a 1% sample of households.  \n",
    "\n",
    "In this case, we are working with file DP-1: Profile of General Population and Housing Characteristics: 2010, which is also a part of something called Summary File 1 (SF-1).  It is for the 100% sample.  A data dictionary for selected fields is below.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P1. POPULATION [1]\n",
    "Total: P1-D001\n",
    "\n",
    "P3. RACE [8]\n",
    "Universe: Total population\n",
    "Total: P3-D001\n",
    "White alone P3-D002\n",
    "Black or African American alone P3-D003\n",
    "American Indian and Alaska Native alone P3-D004\n",
    "Asian alone P3-D005\n",
    "Native Hawaiian and Other Pacific Islander alone P3-D006\n",
    "Some Other Race alone P3-D007\n",
    "Two or More Races P3-D008\n",
    "\n",
    "P4. HISPANIC OR LATINO ORIGIN [3]\n",
    "Universe: Total population\n",
    "Total: P4-D001\n",
    "Not Hispanic or Latino P4-D002\n",
    "Hispanic or Latino P4-D003\n",
    "\n",
    "H1. HOUSING UNITS [1]\n",
    "Universe: Housing units\n",
    "Total H1-D001\n",
    "\n",
    "H3. OCCUPANCY STATUS [3]\n",
    "Universe: Housing units\n",
    "Total: H3-D001\n",
    "Occupied H3-D002\n",
    "Vacant H3-D003\n",
    "\n",
    "H4. TENURE [4]\n",
    "Universe: Occupied housing units\n",
    "Total: H4-D001\n",
    "Owned with a mortgage or a loan H4-D002\n",
    "Owned free and clear H4-D003\n",
    "Renter-occupied H4-D004\n",
    "\n",
    "H5. VACANCY STATUS [8]\n",
    "Universe: Vacant housing units\n",
    "Total: H0050001\n",
    "For rent H0050002\n",
    "Rented, not occupied H0050003\n",
    "For sale only H0050004\n",
    "Sold, not occupied H0050005\n",
    "For seasonal, recreational, or occasional use H0050006\n",
    "For migrant workers H0050007\n",
    "Other vacant H0050008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# skip the second row, which contains descriptions\n",
    "sf1 = pd.read_csv('data/DEC_10_SF1_combined.csv', skiprows=[1])\n",
    "\n",
    "# make the tract, county and state separate\n",
    "sf1['tract'] = sf1['GEO.display-label'].apply(lambda x: x.split(sep=',')[0])\n",
    "sf1['county'] = sf1['GEO.display-label'].apply(lambda x: x.split(sep=',')[1])\n",
    "sf1['state'] = sf1['GEO.display-label'].apply(lambda x: x.split(sep=',')[2])\n",
    "\n",
    "# compute some derived fields\n",
    "sf1['pct_rent'] = sf1['H4-D004'] / sf1['H4-D001'] * 100\n",
    "sf1['pct_black'] = sf1['P3-D003'] / sf1['P3-D001'] * 100\n",
    "sf1['pct_asian'] = sf1['P3-D005'] / sf1['P3-D001'] * 100\n",
    "sf1['pct_white'] = sf1['P3-D002'] / sf1['P3-D001'] * 100\n",
    "sf1['pct_hisp'] = sf1['P4-D003'] / sf1['P4-D001'] * 100\n",
    "sf1['pct_vacant'] = sf1['H5-D001'] / sf1['H1-D001'] * 100\n",
    "sf1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Aggregation Operations\n",
    "\n",
    "Groupby is a powerful method in pandas that follows the split-apply-combine approach to data.  As shown in Figure 9-1 in the context of a sum operation, the data is first split into groups that share the same key values.  Then an operation, in this case a sum, is applied to each group.  Then the results are combined.\n",
    "\n",
    "The built-in aggregation methods available for groupby operations include:\n",
    "* count\n",
    "* sum\n",
    "* mean\n",
    "* median\n",
    "* std, var\n",
    "* min, max\n",
    "* first, last\n",
    "\n",
    "You can also apply your own functions as aggregation methods.\n",
    "\n",
    "![Groupby Operations](groupby.png \"Groupby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this approach to computing total population in each county in our dataset.  We can do this in two steps to help explain what is happening.  First we create a groupby object, using county codes to group all the census blocks in sf1 into groups that share the same county code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = sf1['P1-D001'].groupby(sf1['county'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this grouping object that represents the **split** part of the workflow in the figure above, we can **apply** operations and **combine** the results using methods like sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to capture the result in a DataFrame if we want to use it in other processing, like merging the results to the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_pop = sf1['P1-D001'].groupby(sf1['county']).sum().to_frame(name='county_population')\n",
    "county_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the county total population with sf1 and create a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf2 = pd.merge(sf1,county_pop, left_on='county', right_index=True)\n",
    "sf2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn:  Instead of grouping by the county, group by the state.  What is the total population of Kentucky?  Is there a simple way to check this?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data with Groupby\n",
    "\n",
    "In some cases you may want to apply a function to your data, by group.  An example would be to normalize a column by a mean of each group.  Say we wanted to subtract the mean population density of each county from the population density of each census block. We could write a function to subtract the mean from each value, and then use the transform operation to apply this to each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean(arr):\n",
    "    return arr - arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this tranformation to columns in our dataframe.  As examples, let's 'demean' the pct_black and pct_rent columns, subtracting the county-wide mean of these values from the tract-specific values, so that the result is transformed to have a mean of zero within each county.\n",
    "\n",
    "To check the results, we print the means per county, then the original values for the first 5 rows, then the transformed results.  The transformed results we should be able to calculate by subtracting the appropriate county mean from the tract value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = sf1[['pct_black', 'pct_rent']].groupby(sf1['county']).transform(demean)\n",
    "print(sf1[['pct_black', 'pct_rent']].groupby(sf1['county']).mean())\n",
    "print(sf1[['county','pct_black', 'pct_rent']][:5])\n",
    "print(normalized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge these transformed results on to the original DataFrame, and check the means of the original variables and the tranformed ones.  The transformed ones should be arbitrarily close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf2 = pd.merge(sf1,normalized, left_index=True, right_index=True)\n",
    "\n",
    "sf2.groupby('county')[['pct_black_x', 'pct_black_y', 'pct_rent_x', 'pct_rent_y']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply is a method we have learned previously, which allows us to apply a function to each row in a DataFrame.  We can also combine apply with groupby to apply functions based on group membership.  For example, the function 'top' sorts an array and selects the top n rows from it.  We provide some defaults for the arguments of how many rows, and the column to use for the selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(df, n=5, column='pct_rent'):\n",
    "    return df.sort_values(by=column, ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this on the full dataset and setting the number of rows and the column to get the top values for, in this case using pct_rent to override the default argument, we get the top 10 tracts in the region in terms of percentage rental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top(sf1, n=10, column='pct_rent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we apply this with groupby and use the defaults for n and column, and it applies the function within each county and concatenates the results, producing the top 5 blocks on pop_sqmi for each county in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1.groupby('county').apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass arguments to the function to set n and the column to select the top value from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1.groupby('county').apply(top, n=1, column='P1-D001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Correlations in the Data\n",
    "\n",
    "Pandas provides simple ways of computing correlation coefficients among the columns in your DataFrame.  If you use corr() on a full DF, it will produce a large correlation table.  A bit hard to navigate and you mostly would not be interested in some of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to compute correlation coeffients for a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1[['pct_rent', 'pct_vacant']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this method can be combined with groupby to compute correlation tables by group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1.groupby('county')[['pct_rent', 'pct_vacant']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Your turn to practice:\n",
    "\n",
    "Count the number of census blocks per county.\n",
    "\n",
    "Calculate total households per county.\n",
    "\n",
    "Calculate percent renters by county. (Careful not to calculate the mean percent rental across blocks in a county)\n",
    "\n",
    "Calculate percent vacant by county.\n",
    "\n",
    "Calculate mean, min and max vacancy rate (at the block level) by county.\n",
    "\n",
    "Calculate the 90th percentile of vacancy rate (at the block level) by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
